// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: WAL.proto

package org.apache.hadoop.hbase.protobuf.generated;

public final class WALProtos {
  private WALProtos() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  /**
   * Protobuf enum {@code ScopeType}
   */
  public enum ScopeType
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <code>REPLICATION_SCOPE_LOCAL = 0;</code>
     */
    REPLICATION_SCOPE_LOCAL(0, 0),
    /**
     * <code>REPLICATION_SCOPE_GLOBAL = 1;</code>
     */
    REPLICATION_SCOPE_GLOBAL(1, 1),
    ;

    /**
     * <code>REPLICATION_SCOPE_LOCAL = 0;</code>
     */
    public static final int REPLICATION_SCOPE_LOCAL_VALUE = 0;
    /**
     * <code>REPLICATION_SCOPE_GLOBAL = 1;</code>
     */
    public static final int REPLICATION_SCOPE_GLOBAL_VALUE = 1;


    public final int getNumber() { return value; }

    public static ScopeType valueOf(int value) {
      switch (value) {
        case 0: return REPLICATION_SCOPE_LOCAL;
        case 1: return REPLICATION_SCOPE_GLOBAL;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<ScopeType>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static com.google.protobuf.Internal.EnumLiteMap<ScopeType>
        internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<ScopeType>() {
            public ScopeType findValueByNumber(int number) {
              return ScopeType.valueOf(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(index);
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.getDescriptor().getEnumTypes().get(0);
    }

    private static final ScopeType[] VALUES = values();

    public static ScopeType valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int index;
    private final int value;

    private ScopeType(int index, int value) {
      this.index = index;
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:ScopeType)
  }

  public interface WALHeaderOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional bool has_compression = 1;
    /**
     * <code>optional bool has_compression = 1;</code>
     */
    boolean hasHasCompression();
    /**
     * <code>optional bool has_compression = 1;</code>
     */
    boolean getHasCompression();

    // optional bytes encryption_key = 2;
    /**
     * <code>optional bytes encryption_key = 2;</code>
     */
    boolean hasEncryptionKey();
    /**
     * <code>optional bytes encryption_key = 2;</code>
     */
    com.google.protobuf.ByteString getEncryptionKey();

    // optional bool has_tag_compression = 3;
    /**
     * <code>optional bool has_tag_compression = 3;</code>
     */
    boolean hasHasTagCompression();
    /**
     * <code>optional bool has_tag_compression = 3;</code>
     */
    boolean getHasTagCompression();
  }
  /**
   * Protobuf type {@code WALHeader}
   */
  public static final class WALHeader extends
      com.google.protobuf.GeneratedMessage
      implements WALHeaderOrBuilder {
    // Use WALHeader.newBuilder() to construct.
    private WALHeader(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private WALHeader(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final WALHeader defaultInstance;
    public static WALHeader getDefaultInstance() {
      return defaultInstance;
    }

    public WALHeader getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private WALHeader(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              hasCompression_ = input.readBool();
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              encryptionKey_ = input.readBytes();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              hasTagCompression_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_WALHeader_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_WALHeader_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader.Builder.class);
    }

    public static com.google.protobuf.Parser<WALHeader> PARSER =
        new com.google.protobuf.AbstractParser<WALHeader>() {
      public WALHeader parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new WALHeader(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<WALHeader> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional bool has_compression = 1;
    public static final int HAS_COMPRESSION_FIELD_NUMBER = 1;
    private boolean hasCompression_;
    /**
     * <code>optional bool has_compression = 1;</code>
     */
    public boolean hasHasCompression() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional bool has_compression = 1;</code>
     */
    public boolean getHasCompression() {
      return hasCompression_;
    }

    // optional bytes encryption_key = 2;
    public static final int ENCRYPTION_KEY_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString encryptionKey_;
    /**
     * <code>optional bytes encryption_key = 2;</code>
     */
    public boolean hasEncryptionKey() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional bytes encryption_key = 2;</code>
     */
    public com.google.protobuf.ByteString getEncryptionKey() {
      return encryptionKey_;
    }

    // optional bool has_tag_compression = 3;
    public static final int HAS_TAG_COMPRESSION_FIELD_NUMBER = 3;
    private boolean hasTagCompression_;
    /**
     * <code>optional bool has_tag_compression = 3;</code>
     */
    public boolean hasHasTagCompression() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional bool has_tag_compression = 3;</code>
     */
    public boolean getHasTagCompression() {
      return hasTagCompression_;
    }

    private void initFields() {
      hasCompression_ = false;
      encryptionKey_ = com.google.protobuf.ByteString.EMPTY;
      hasTagCompression_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, hasCompression_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, encryptionKey_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBool(3, hasTagCompression_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, hasCompression_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, encryptionKey_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(3, hasTagCompression_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader other = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader) obj;

      boolean result = true;
      result = result && (hasHasCompression() == other.hasHasCompression());
      if (hasHasCompression()) {
        result = result && (getHasCompression()
            == other.getHasCompression());
      }
      result = result && (hasEncryptionKey() == other.hasEncryptionKey());
      if (hasEncryptionKey()) {
        result = result && getEncryptionKey()
            .equals(other.getEncryptionKey());
      }
      result = result && (hasHasTagCompression() == other.hasHasTagCompression());
      if (hasHasTagCompression()) {
        result = result && (getHasTagCompression()
            == other.getHasTagCompression());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasHasCompression()) {
        hash = (37 * hash) + HAS_COMPRESSION_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getHasCompression());
      }
      if (hasEncryptionKey()) {
        hash = (37 * hash) + ENCRYPTION_KEY_FIELD_NUMBER;
        hash = (53 * hash) + getEncryptionKey().hashCode();
      }
      if (hasHasTagCompression()) {
        hash = (37 * hash) + HAS_TAG_COMPRESSION_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getHasTagCompression());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code WALHeader}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeaderOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_WALHeader_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_WALHeader_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        hasCompression_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        encryptionKey_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        hasTagCompression_ = false;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_WALHeader_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader build() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader result = new org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.hasCompression_ = hasCompression_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.encryptionKey_ = encryptionKey_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.hasTagCompression_ = hasTagCompression_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader.getDefaultInstance()) return this;
        if (other.hasHasCompression()) {
          setHasCompression(other.getHasCompression());
        }
        if (other.hasEncryptionKey()) {
          setEncryptionKey(other.getEncryptionKey());
        }
        if (other.hasHasTagCompression()) {
          setHasTagCompression(other.getHasTagCompression());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALHeader) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional bool has_compression = 1;
      private boolean hasCompression_ ;
      /**
       * <code>optional bool has_compression = 1;</code>
       */
      public boolean hasHasCompression() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional bool has_compression = 1;</code>
       */
      public boolean getHasCompression() {
        return hasCompression_;
      }
      /**
       * <code>optional bool has_compression = 1;</code>
       */
      public Builder setHasCompression(boolean value) {
        bitField0_ |= 0x00000001;
        hasCompression_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool has_compression = 1;</code>
       */
      public Builder clearHasCompression() {
        bitField0_ = (bitField0_ & ~0x00000001);
        hasCompression_ = false;
        onChanged();
        return this;
      }

      // optional bytes encryption_key = 2;
      private com.google.protobuf.ByteString encryptionKey_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>optional bytes encryption_key = 2;</code>
       */
      public boolean hasEncryptionKey() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional bytes encryption_key = 2;</code>
       */
      public com.google.protobuf.ByteString getEncryptionKey() {
        return encryptionKey_;
      }
      /**
       * <code>optional bytes encryption_key = 2;</code>
       */
      public Builder setEncryptionKey(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        encryptionKey_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bytes encryption_key = 2;</code>
       */
      public Builder clearEncryptionKey() {
        bitField0_ = (bitField0_ & ~0x00000002);
        encryptionKey_ = getDefaultInstance().getEncryptionKey();
        onChanged();
        return this;
      }

      // optional bool has_tag_compression = 3;
      private boolean hasTagCompression_ ;
      /**
       * <code>optional bool has_tag_compression = 3;</code>
       */
      public boolean hasHasTagCompression() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional bool has_tag_compression = 3;</code>
       */
      public boolean getHasTagCompression() {
        return hasTagCompression_;
      }
      /**
       * <code>optional bool has_tag_compression = 3;</code>
       */
      public Builder setHasTagCompression(boolean value) {
        bitField0_ |= 0x00000004;
        hasTagCompression_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool has_tag_compression = 3;</code>
       */
      public Builder clearHasTagCompression() {
        bitField0_ = (bitField0_ & ~0x00000004);
        hasTagCompression_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:WALHeader)
    }

    static {
      defaultInstance = new WALHeader(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:WALHeader)
  }

  public interface WALKeyOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required bytes encoded_region_name = 1;
    /**
     * <code>required bytes encoded_region_name = 1;</code>
     */
    boolean hasEncodedRegionName();
    /**
     * <code>required bytes encoded_region_name = 1;</code>
     */
    com.google.protobuf.ByteString getEncodedRegionName();

    // required bytes table_name = 2;
    /**
     * <code>required bytes table_name = 2;</code>
     */
    boolean hasTableName();
    /**
     * <code>required bytes table_name = 2;</code>
     */
    com.google.protobuf.ByteString getTableName();

    // required uint64 log_sequence_number = 3;
    /**
     * <code>required uint64 log_sequence_number = 3;</code>
     */
    boolean hasLogSequenceNumber();
    /**
     * <code>required uint64 log_sequence_number = 3;</code>
     */
    long getLogSequenceNumber();

    // required uint64 write_time = 4;
    /**
     * <code>required uint64 write_time = 4;</code>
     */
    boolean hasWriteTime();
    /**
     * <code>required uint64 write_time = 4;</code>
     */
    long getWriteTime();

    // optional .UUID cluster_id = 5 [deprecated = true];
    /**
     * <code>optional .UUID cluster_id = 5 [deprecated = true];</code>
     *
     * <pre>
     *
     *This parameter is deprecated in favor of clusters which 
     *contains the list of clusters that have consumed the change.
     *It is retained so that the log created by earlier releases (0.94) 
     *can be read by the newer releases.
     * </pre>
     */
    @java.lang.Deprecated boolean hasClusterId();
    /**
     * <code>optional .UUID cluster_id = 5 [deprecated = true];</code>
     *
     * <pre>
     *
     *This parameter is deprecated in favor of clusters which 
     *contains the list of clusters that have consumed the change.
     *It is retained so that the log created by earlier releases (0.94) 
     *can be read by the newer releases.
     * </pre>
     */
    @java.lang.Deprecated org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID getClusterId();
    /**
     * <code>optional .UUID cluster_id = 5 [deprecated = true];</code>
     *
     * <pre>
     *
     *This parameter is deprecated in favor of clusters which 
     *contains the list of clusters that have consumed the change.
     *It is retained so that the log created by earlier releases (0.94) 
     *can be read by the newer releases.
     * </pre>
     */
    @java.lang.Deprecated org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder getClusterIdOrBuilder();

    // repeated .FamilyScope scopes = 6;
    /**
     * <code>repeated .FamilyScope scopes = 6;</code>
     */
    java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope> 
        getScopesList();
    /**
     * <code>repeated .FamilyScope scopes = 6;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope getScopes(int index);
    /**
     * <code>repeated .FamilyScope scopes = 6;</code>
     */
    int getScopesCount();
    /**
     * <code>repeated .FamilyScope scopes = 6;</code>
     */
    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder> 
        getScopesOrBuilderList();
    /**
     * <code>repeated .FamilyScope scopes = 6;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder getScopesOrBuilder(
        int index);

    // optional uint32 following_kv_count = 7;
    /**
     * <code>optional uint32 following_kv_count = 7;</code>
     */
    boolean hasFollowingKvCount();
    /**
     * <code>optional uint32 following_kv_count = 7;</code>
     */
    int getFollowingKvCount();

    // repeated .UUID cluster_ids = 8;
    /**
     * <code>repeated .UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID> 
        getClusterIdsList();
    /**
     * <code>repeated .UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID getClusterIds(int index);
    /**
     * <code>repeated .UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    int getClusterIdsCount();
    /**
     * <code>repeated .UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder> 
        getClusterIdsOrBuilderList();
    /**
     * <code>repeated .UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder getClusterIdsOrBuilder(
        int index);

    // optional uint64 nonceGroup = 9;
    /**
     * <code>optional uint64 nonceGroup = 9;</code>
     */
    boolean hasNonceGroup();
    /**
     * <code>optional uint64 nonceGroup = 9;</code>
     */
    long getNonceGroup();

    // optional uint64 nonce = 10;
    /**
     * <code>optional uint64 nonce = 10;</code>
     */
    boolean hasNonce();
    /**
     * <code>optional uint64 nonce = 10;</code>
     */
    long getNonce();
  }
  /**
   * Protobuf type {@code WALKey}
   *
   * <pre>
   * Protocol buffer version of HLogKey; see HLogKey comment, not really a key but WALEdit header for some KVs
   * </pre>
   */
  public static final class WALKey extends
      com.google.protobuf.GeneratedMessage
      implements WALKeyOrBuilder {
    // Use WALKey.newBuilder() to construct.
    private WALKey(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private WALKey(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final WALKey defaultInstance;
    public static WALKey getDefaultInstance() {
      return defaultInstance;
    }

    public WALKey getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private WALKey(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              encodedRegionName_ = input.readBytes();
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              tableName_ = input.readBytes();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              logSequenceNumber_ = input.readUInt64();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              writeTime_ = input.readUInt64();
              break;
            }
            case 42: {
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder subBuilder = null;
              if (((bitField0_ & 0x00000010) == 0x00000010)) {
                subBuilder = clusterId_.toBuilder();
              }
              clusterId_ = input.readMessage(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(clusterId_);
                clusterId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000010;
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
                scopes_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope>();
                mutable_bitField0_ |= 0x00000020;
              }
              scopes_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.PARSER, extensionRegistry));
              break;
            }
            case 56: {
              bitField0_ |= 0x00000020;
              followingKvCount_ = input.readUInt32();
              break;
            }
            case 66: {
              if (!((mutable_bitField0_ & 0x00000080) == 0x00000080)) {
                clusterIds_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID>();
                mutable_bitField0_ |= 0x00000080;
              }
              clusterIds_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.PARSER, extensionRegistry));
              break;
            }
            case 72: {
              bitField0_ |= 0x00000040;
              nonceGroup_ = input.readUInt64();
              break;
            }
            case 80: {
              bitField0_ |= 0x00000080;
              nonce_ = input.readUInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
          scopes_ = java.util.Collections.unmodifiableList(scopes_);
        }
        if (((mutable_bitField0_ & 0x00000080) == 0x00000080)) {
          clusterIds_ = java.util.Collections.unmodifiableList(clusterIds_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_WALKey_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_WALKey_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey.Builder.class);
    }

    public static com.google.protobuf.Parser<WALKey> PARSER =
        new com.google.protobuf.AbstractParser<WALKey>() {
      public WALKey parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new WALKey(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<WALKey> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required bytes encoded_region_name = 1;
    public static final int ENCODED_REGION_NAME_FIELD_NUMBER = 1;
    private com.google.protobuf.ByteString encodedRegionName_;
    /**
     * <code>required bytes encoded_region_name = 1;</code>
     */
    public boolean hasEncodedRegionName() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required bytes encoded_region_name = 1;</code>
     */
    public com.google.protobuf.ByteString getEncodedRegionName() {
      return encodedRegionName_;
    }

    // required bytes table_name = 2;
    public static final int TABLE_NAME_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString tableName_;
    /**
     * <code>required bytes table_name = 2;</code>
     */
    public boolean hasTableName() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required bytes table_name = 2;</code>
     */
    public com.google.protobuf.ByteString getTableName() {
      return tableName_;
    }

    // required uint64 log_sequence_number = 3;
    public static final int LOG_SEQUENCE_NUMBER_FIELD_NUMBER = 3;
    private long logSequenceNumber_;
    /**
     * <code>required uint64 log_sequence_number = 3;</code>
     */
    public boolean hasLogSequenceNumber() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>required uint64 log_sequence_number = 3;</code>
     */
    public long getLogSequenceNumber() {
      return logSequenceNumber_;
    }

    // required uint64 write_time = 4;
    public static final int WRITE_TIME_FIELD_NUMBER = 4;
    private long writeTime_;
    /**
     * <code>required uint64 write_time = 4;</code>
     */
    public boolean hasWriteTime() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>required uint64 write_time = 4;</code>
     */
    public long getWriteTime() {
      return writeTime_;
    }

    // optional .UUID cluster_id = 5 [deprecated = true];
    public static final int CLUSTER_ID_FIELD_NUMBER = 5;
    private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID clusterId_;
    /**
     * <code>optional .UUID cluster_id = 5 [deprecated = true];</code>
     *
     * <pre>
     *
     *This parameter is deprecated in favor of clusters which 
     *contains the list of clusters that have consumed the change.
     *It is retained so that the log created by earlier releases (0.94) 
     *can be read by the newer releases.
     * </pre>
     */
    @java.lang.Deprecated public boolean hasClusterId() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional .UUID cluster_id = 5 [deprecated = true];</code>
     *
     * <pre>
     *
     *This parameter is deprecated in favor of clusters which 
     *contains the list of clusters that have consumed the change.
     *It is retained so that the log created by earlier releases (0.94) 
     *can be read by the newer releases.
     * </pre>
     */
    @java.lang.Deprecated public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID getClusterId() {
      return clusterId_;
    }
    /**
     * <code>optional .UUID cluster_id = 5 [deprecated = true];</code>
     *
     * <pre>
     *
     *This parameter is deprecated in favor of clusters which 
     *contains the list of clusters that have consumed the change.
     *It is retained so that the log created by earlier releases (0.94) 
     *can be read by the newer releases.
     * </pre>
     */
    @java.lang.Deprecated public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder getClusterIdOrBuilder() {
      return clusterId_;
    }

    // repeated .FamilyScope scopes = 6;
    public static final int SCOPES_FIELD_NUMBER = 6;
    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope> scopes_;
    /**
     * <code>repeated .FamilyScope scopes = 6;</code>
     */
    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope> getScopesList() {
      return scopes_;
    }
    /**
     * <code>repeated .FamilyScope scopes = 6;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder> 
        getScopesOrBuilderList() {
      return scopes_;
    }
    /**
     * <code>repeated .FamilyScope scopes = 6;</code>
     */
    public int getScopesCount() {
      return scopes_.size();
    }
    /**
     * <code>repeated .FamilyScope scopes = 6;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope getScopes(int index) {
      return scopes_.get(index);
    }
    /**
     * <code>repeated .FamilyScope scopes = 6;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder getScopesOrBuilder(
        int index) {
      return scopes_.get(index);
    }

    // optional uint32 following_kv_count = 7;
    public static final int FOLLOWING_KV_COUNT_FIELD_NUMBER = 7;
    private int followingKvCount_;
    /**
     * <code>optional uint32 following_kv_count = 7;</code>
     */
    public boolean hasFollowingKvCount() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    /**
     * <code>optional uint32 following_kv_count = 7;</code>
     */
    public int getFollowingKvCount() {
      return followingKvCount_;
    }

    // repeated .UUID cluster_ids = 8;
    public static final int CLUSTER_IDS_FIELD_NUMBER = 8;
    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID> clusterIds_;
    /**
     * <code>repeated .UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID> getClusterIdsList() {
      return clusterIds_;
    }
    /**
     * <code>repeated .UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder> 
        getClusterIdsOrBuilderList() {
      return clusterIds_;
    }
    /**
     * <code>repeated .UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    public int getClusterIdsCount() {
      return clusterIds_.size();
    }
    /**
     * <code>repeated .UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID getClusterIds(int index) {
      return clusterIds_.get(index);
    }
    /**
     * <code>repeated .UUID cluster_ids = 8;</code>
     *
     * <pre>
     *
     *This field contains the list of clusters that have
     *consumed the change
     * </pre>
     */
    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder getClusterIdsOrBuilder(
        int index) {
      return clusterIds_.get(index);
    }

    // optional uint64 nonceGroup = 9;
    public static final int NONCEGROUP_FIELD_NUMBER = 9;
    private long nonceGroup_;
    /**
     * <code>optional uint64 nonceGroup = 9;</code>
     */
    public boolean hasNonceGroup() {
      return ((bitField0_ & 0x00000040) == 0x00000040);
    }
    /**
     * <code>optional uint64 nonceGroup = 9;</code>
     */
    public long getNonceGroup() {
      return nonceGroup_;
    }

    // optional uint64 nonce = 10;
    public static final int NONCE_FIELD_NUMBER = 10;
    private long nonce_;
    /**
     * <code>optional uint64 nonce = 10;</code>
     */
    public boolean hasNonce() {
      return ((bitField0_ & 0x00000080) == 0x00000080);
    }
    /**
     * <code>optional uint64 nonce = 10;</code>
     */
    public long getNonce() {
      return nonce_;
    }

    private void initFields() {
      encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
      tableName_ = com.google.protobuf.ByteString.EMPTY;
      logSequenceNumber_ = 0L;
      writeTime_ = 0L;
      clusterId_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance();
      scopes_ = java.util.Collections.emptyList();
      followingKvCount_ = 0;
      clusterIds_ = java.util.Collections.emptyList();
      nonceGroup_ = 0L;
      nonce_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasEncodedRegionName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasTableName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasLogSequenceNumber()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasWriteTime()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (hasClusterId()) {
        if (!getClusterId().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getScopesCount(); i++) {
        if (!getScopes(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getClusterIdsCount(); i++) {
        if (!getClusterIds(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, encodedRegionName_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, tableName_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt64(3, logSequenceNumber_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeUInt64(4, writeTime_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeMessage(5, clusterId_);
      }
      for (int i = 0; i < scopes_.size(); i++) {
        output.writeMessage(6, scopes_.get(i));
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeUInt32(7, followingKvCount_);
      }
      for (int i = 0; i < clusterIds_.size(); i++) {
        output.writeMessage(8, clusterIds_.get(i));
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        output.writeUInt64(9, nonceGroup_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        output.writeUInt64(10, nonce_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, encodedRegionName_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, tableName_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, logSequenceNumber_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(4, writeTime_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, clusterId_);
      }
      for (int i = 0; i < scopes_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, scopes_.get(i));
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(7, followingKvCount_);
      }
      for (int i = 0; i < clusterIds_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(8, clusterIds_.get(i));
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(9, nonceGroup_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(10, nonce_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey other = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey) obj;

      boolean result = true;
      result = result && (hasEncodedRegionName() == other.hasEncodedRegionName());
      if (hasEncodedRegionName()) {
        result = result && getEncodedRegionName()
            .equals(other.getEncodedRegionName());
      }
      result = result && (hasTableName() == other.hasTableName());
      if (hasTableName()) {
        result = result && getTableName()
            .equals(other.getTableName());
      }
      result = result && (hasLogSequenceNumber() == other.hasLogSequenceNumber());
      if (hasLogSequenceNumber()) {
        result = result && (getLogSequenceNumber()
            == other.getLogSequenceNumber());
      }
      result = result && (hasWriteTime() == other.hasWriteTime());
      if (hasWriteTime()) {
        result = result && (getWriteTime()
            == other.getWriteTime());
      }
      result = result && (hasClusterId() == other.hasClusterId());
      if (hasClusterId()) {
        result = result && getClusterId()
            .equals(other.getClusterId());
      }
      result = result && getScopesList()
          .equals(other.getScopesList());
      result = result && (hasFollowingKvCount() == other.hasFollowingKvCount());
      if (hasFollowingKvCount()) {
        result = result && (getFollowingKvCount()
            == other.getFollowingKvCount());
      }
      result = result && getClusterIdsList()
          .equals(other.getClusterIdsList());
      result = result && (hasNonceGroup() == other.hasNonceGroup());
      if (hasNonceGroup()) {
        result = result && (getNonceGroup()
            == other.getNonceGroup());
      }
      result = result && (hasNonce() == other.hasNonce());
      if (hasNonce()) {
        result = result && (getNonce()
            == other.getNonce());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasEncodedRegionName()) {
        hash = (37 * hash) + ENCODED_REGION_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getEncodedRegionName().hashCode();
      }
      if (hasTableName()) {
        hash = (37 * hash) + TABLE_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getTableName().hashCode();
      }
      if (hasLogSequenceNumber()) {
        hash = (37 * hash) + LOG_SEQUENCE_NUMBER_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getLogSequenceNumber());
      }
      if (hasWriteTime()) {
        hash = (37 * hash) + WRITE_TIME_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getWriteTime());
      }
      if (hasClusterId()) {
        hash = (37 * hash) + CLUSTER_ID_FIELD_NUMBER;
        hash = (53 * hash) + getClusterId().hashCode();
      }
      if (getScopesCount() > 0) {
        hash = (37 * hash) + SCOPES_FIELD_NUMBER;
        hash = (53 * hash) + getScopesList().hashCode();
      }
      if (hasFollowingKvCount()) {
        hash = (37 * hash) + FOLLOWING_KV_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + getFollowingKvCount();
      }
      if (getClusterIdsCount() > 0) {
        hash = (37 * hash) + CLUSTER_IDS_FIELD_NUMBER;
        hash = (53 * hash) + getClusterIdsList().hashCode();
      }
      if (hasNonceGroup()) {
        hash = (37 * hash) + NONCEGROUP_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getNonceGroup());
      }
      if (hasNonce()) {
        hash = (37 * hash) + NONCE_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getNonce());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code WALKey}
     *
     * <pre>
     * Protocol buffer version of HLogKey; see HLogKey comment, not really a key but WALEdit header for some KVs
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKeyOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_WALKey_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_WALKey_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getClusterIdFieldBuilder();
          getScopesFieldBuilder();
          getClusterIdsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        tableName_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        logSequenceNumber_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        writeTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        if (clusterIdBuilder_ == null) {
          clusterId_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance();
        } else {
          clusterIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        if (scopesBuilder_ == null) {
          scopes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
        } else {
          scopesBuilder_.clear();
        }
        followingKvCount_ = 0;
        bitField0_ = (bitField0_ & ~0x00000040);
        if (clusterIdsBuilder_ == null) {
          clusterIds_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000080);
        } else {
          clusterIdsBuilder_.clear();
        }
        nonceGroup_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000100);
        nonce_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000200);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_WALKey_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey build() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey result = new org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.encodedRegionName_ = encodedRegionName_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.tableName_ = tableName_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.logSequenceNumber_ = logSequenceNumber_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.writeTime_ = writeTime_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        if (clusterIdBuilder_ == null) {
          result.clusterId_ = clusterId_;
        } else {
          result.clusterId_ = clusterIdBuilder_.build();
        }
        if (scopesBuilder_ == null) {
          if (((bitField0_ & 0x00000020) == 0x00000020)) {
            scopes_ = java.util.Collections.unmodifiableList(scopes_);
            bitField0_ = (bitField0_ & ~0x00000020);
          }
          result.scopes_ = scopes_;
        } else {
          result.scopes_ = scopesBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000020;
        }
        result.followingKvCount_ = followingKvCount_;
        if (clusterIdsBuilder_ == null) {
          if (((bitField0_ & 0x00000080) == 0x00000080)) {
            clusterIds_ = java.util.Collections.unmodifiableList(clusterIds_);
            bitField0_ = (bitField0_ & ~0x00000080);
          }
          result.clusterIds_ = clusterIds_;
        } else {
          result.clusterIds_ = clusterIdsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
          to_bitField0_ |= 0x00000040;
        }
        result.nonceGroup_ = nonceGroup_;
        if (((from_bitField0_ & 0x00000200) == 0x00000200)) {
          to_bitField0_ |= 0x00000080;
        }
        result.nonce_ = nonce_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey.getDefaultInstance()) return this;
        if (other.hasEncodedRegionName()) {
          setEncodedRegionName(other.getEncodedRegionName());
        }
        if (other.hasTableName()) {
          setTableName(other.getTableName());
        }
        if (other.hasLogSequenceNumber()) {
          setLogSequenceNumber(other.getLogSequenceNumber());
        }
        if (other.hasWriteTime()) {
          setWriteTime(other.getWriteTime());
        }
        if (other.hasClusterId()) {
          mergeClusterId(other.getClusterId());
        }
        if (scopesBuilder_ == null) {
          if (!other.scopes_.isEmpty()) {
            if (scopes_.isEmpty()) {
              scopes_ = other.scopes_;
              bitField0_ = (bitField0_ & ~0x00000020);
            } else {
              ensureScopesIsMutable();
              scopes_.addAll(other.scopes_);
            }
            onChanged();
          }
        } else {
          if (!other.scopes_.isEmpty()) {
            if (scopesBuilder_.isEmpty()) {
              scopesBuilder_.dispose();
              scopesBuilder_ = null;
              scopes_ = other.scopes_;
              bitField0_ = (bitField0_ & ~0x00000020);
              scopesBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getScopesFieldBuilder() : null;
            } else {
              scopesBuilder_.addAllMessages(other.scopes_);
            }
          }
        }
        if (other.hasFollowingKvCount()) {
          setFollowingKvCount(other.getFollowingKvCount());
        }
        if (clusterIdsBuilder_ == null) {
          if (!other.clusterIds_.isEmpty()) {
            if (clusterIds_.isEmpty()) {
              clusterIds_ = other.clusterIds_;
              bitField0_ = (bitField0_ & ~0x00000080);
            } else {
              ensureClusterIdsIsMutable();
              clusterIds_.addAll(other.clusterIds_);
            }
            onChanged();
          }
        } else {
          if (!other.clusterIds_.isEmpty()) {
            if (clusterIdsBuilder_.isEmpty()) {
              clusterIdsBuilder_.dispose();
              clusterIdsBuilder_ = null;
              clusterIds_ = other.clusterIds_;
              bitField0_ = (bitField0_ & ~0x00000080);
              clusterIdsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getClusterIdsFieldBuilder() : null;
            } else {
              clusterIdsBuilder_.addAllMessages(other.clusterIds_);
            }
          }
        }
        if (other.hasNonceGroup()) {
          setNonceGroup(other.getNonceGroup());
        }
        if (other.hasNonce()) {
          setNonce(other.getNonce());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasEncodedRegionName()) {
          
          return false;
        }
        if (!hasTableName()) {
          
          return false;
        }
        if (!hasLogSequenceNumber()) {
          
          return false;
        }
        if (!hasWriteTime()) {
          
          return false;
        }
        if (hasClusterId()) {
          if (!getClusterId().isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getScopesCount(); i++) {
          if (!getScopes(i).isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getClusterIdsCount(); i++) {
          if (!getClusterIds(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALKey) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required bytes encoded_region_name = 1;
      private com.google.protobuf.ByteString encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes encoded_region_name = 1;</code>
       */
      public boolean hasEncodedRegionName() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required bytes encoded_region_name = 1;</code>
       */
      public com.google.protobuf.ByteString getEncodedRegionName() {
        return encodedRegionName_;
      }
      /**
       * <code>required bytes encoded_region_name = 1;</code>
       */
      public Builder setEncodedRegionName(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        encodedRegionName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes encoded_region_name = 1;</code>
       */
      public Builder clearEncodedRegionName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        encodedRegionName_ = getDefaultInstance().getEncodedRegionName();
        onChanged();
        return this;
      }

      // required bytes table_name = 2;
      private com.google.protobuf.ByteString tableName_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes table_name = 2;</code>
       */
      public boolean hasTableName() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required bytes table_name = 2;</code>
       */
      public com.google.protobuf.ByteString getTableName() {
        return tableName_;
      }
      /**
       * <code>required bytes table_name = 2;</code>
       */
      public Builder setTableName(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        tableName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes table_name = 2;</code>
       */
      public Builder clearTableName() {
        bitField0_ = (bitField0_ & ~0x00000002);
        tableName_ = getDefaultInstance().getTableName();
        onChanged();
        return this;
      }

      // required uint64 log_sequence_number = 3;
      private long logSequenceNumber_ ;
      /**
       * <code>required uint64 log_sequence_number = 3;</code>
       */
      public boolean hasLogSequenceNumber() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>required uint64 log_sequence_number = 3;</code>
       */
      public long getLogSequenceNumber() {
        return logSequenceNumber_;
      }
      /**
       * <code>required uint64 log_sequence_number = 3;</code>
       */
      public Builder setLogSequenceNumber(long value) {
        bitField0_ |= 0x00000004;
        logSequenceNumber_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 log_sequence_number = 3;</code>
       */
      public Builder clearLogSequenceNumber() {
        bitField0_ = (bitField0_ & ~0x00000004);
        logSequenceNumber_ = 0L;
        onChanged();
        return this;
      }

      // required uint64 write_time = 4;
      private long writeTime_ ;
      /**
       * <code>required uint64 write_time = 4;</code>
       */
      public boolean hasWriteTime() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>required uint64 write_time = 4;</code>
       */
      public long getWriteTime() {
        return writeTime_;
      }
      /**
       * <code>required uint64 write_time = 4;</code>
       */
      public Builder setWriteTime(long value) {
        bitField0_ |= 0x00000008;
        writeTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required uint64 write_time = 4;</code>
       */
      public Builder clearWriteTime() {
        bitField0_ = (bitField0_ & ~0x00000008);
        writeTime_ = 0L;
        onChanged();
        return this;
      }

      // optional .UUID cluster_id = 5 [deprecated = true];
      private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID clusterId_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder> clusterIdBuilder_;
      /**
       * <code>optional .UUID cluster_id = 5 [deprecated = true];</code>
       *
       * <pre>
       *
       *This parameter is deprecated in favor of clusters which 
       *contains the list of clusters that have consumed the change.
       *It is retained so that the log created by earlier releases (0.94) 
       *can be read by the newer releases.
       * </pre>
       */
      @java.lang.Deprecated public boolean hasClusterId() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional .UUID cluster_id = 5 [deprecated = true];</code>
       *
       * <pre>
       *
       *This parameter is deprecated in favor of clusters which 
       *contains the list of clusters that have consumed the change.
       *It is retained so that the log created by earlier releases (0.94) 
       *can be read by the newer releases.
       * </pre>
       */
      @java.lang.Deprecated public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID getClusterId() {
        if (clusterIdBuilder_ == null) {
          return clusterId_;
        } else {
          return clusterIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .UUID cluster_id = 5 [deprecated = true];</code>
       *
       * <pre>
       *
       *This parameter is deprecated in favor of clusters which 
       *contains the list of clusters that have consumed the change.
       *It is retained so that the log created by earlier releases (0.94) 
       *can be read by the newer releases.
       * </pre>
       */
      @java.lang.Deprecated public Builder setClusterId(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID value) {
        if (clusterIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          clusterId_ = value;
          onChanged();
        } else {
          clusterIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .UUID cluster_id = 5 [deprecated = true];</code>
       *
       * <pre>
       *
       *This parameter is deprecated in favor of clusters which 
       *contains the list of clusters that have consumed the change.
       *It is retained so that the log created by earlier releases (0.94) 
       *can be read by the newer releases.
       * </pre>
       */
      @java.lang.Deprecated public Builder setClusterId(
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder builderForValue) {
        if (clusterIdBuilder_ == null) {
          clusterId_ = builderForValue.build();
          onChanged();
        } else {
          clusterIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .UUID cluster_id = 5 [deprecated = true];</code>
       *
       * <pre>
       *
       *This parameter is deprecated in favor of clusters which 
       *contains the list of clusters that have consumed the change.
       *It is retained so that the log created by earlier releases (0.94) 
       *can be read by the newer releases.
       * </pre>
       */
      @java.lang.Deprecated public Builder mergeClusterId(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID value) {
        if (clusterIdBuilder_ == null) {
          if (((bitField0_ & 0x00000010) == 0x00000010) &&
              clusterId_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance()) {
            clusterId_ =
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.newBuilder(clusterId_).mergeFrom(value).buildPartial();
          } else {
            clusterId_ = value;
          }
          onChanged();
        } else {
          clusterIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .UUID cluster_id = 5 [deprecated = true];</code>
       *
       * <pre>
       *
       *This parameter is deprecated in favor of clusters which 
       *contains the list of clusters that have consumed the change.
       *It is retained so that the log created by earlier releases (0.94) 
       *can be read by the newer releases.
       * </pre>
       */
      @java.lang.Deprecated public Builder clearClusterId() {
        if (clusterIdBuilder_ == null) {
          clusterId_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance();
          onChanged();
        } else {
          clusterIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }
      /**
       * <code>optional .UUID cluster_id = 5 [deprecated = true];</code>
       *
       * <pre>
       *
       *This parameter is deprecated in favor of clusters which 
       *contains the list of clusters that have consumed the change.
       *It is retained so that the log created by earlier releases (0.94) 
       *can be read by the newer releases.
       * </pre>
       */
      @java.lang.Deprecated public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder getClusterIdBuilder() {
        bitField0_ |= 0x00000010;
        onChanged();
        return getClusterIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .UUID cluster_id = 5 [deprecated = true];</code>
       *
       * <pre>
       *
       *This parameter is deprecated in favor of clusters which 
       *contains the list of clusters that have consumed the change.
       *It is retained so that the log created by earlier releases (0.94) 
       *can be read by the newer releases.
       * </pre>
       */
      @java.lang.Deprecated public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder getClusterIdOrBuilder() {
        if (clusterIdBuilder_ != null) {
          return clusterIdBuilder_.getMessageOrBuilder();
        } else {
          return clusterId_;
        }
      }
      /**
       * <code>optional .UUID cluster_id = 5 [deprecated = true];</code>
       *
       * <pre>
       *
       *This parameter is deprecated in favor of clusters which 
       *contains the list of clusters that have consumed the change.
       *It is retained so that the log created by earlier releases (0.94) 
       *can be read by the newer releases.
       * </pre>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder> 
          getClusterIdFieldBuilder() {
        if (clusterIdBuilder_ == null) {
          clusterIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder>(
                  clusterId_,
                  getParentForChildren(),
                  isClean());
          clusterId_ = null;
        }
        return clusterIdBuilder_;
      }

      // repeated .FamilyScope scopes = 6;
      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope> scopes_ =
        java.util.Collections.emptyList();
      private void ensureScopesIsMutable() {
        if (!((bitField0_ & 0x00000020) == 0x00000020)) {
          scopes_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope>(scopes_);
          bitField0_ |= 0x00000020;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder> scopesBuilder_;

      /**
       * <code>repeated .FamilyScope scopes = 6;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope> getScopesList() {
        if (scopesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(scopes_);
        } else {
          return scopesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .FamilyScope scopes = 6;</code>
       */
      public int getScopesCount() {
        if (scopesBuilder_ == null) {
          return scopes_.size();
        } else {
          return scopesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .FamilyScope scopes = 6;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope getScopes(int index) {
        if (scopesBuilder_ == null) {
          return scopes_.get(index);
        } else {
          return scopesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .FamilyScope scopes = 6;</code>
       */
      public Builder setScopes(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope value) {
        if (scopesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureScopesIsMutable();
          scopes_.set(index, value);
          onChanged();
        } else {
          scopesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .FamilyScope scopes = 6;</code>
       */
      public Builder setScopes(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder builderForValue) {
        if (scopesBuilder_ == null) {
          ensureScopesIsMutable();
          scopes_.set(index, builderForValue.build());
          onChanged();
        } else {
          scopesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .FamilyScope scopes = 6;</code>
       */
      public Builder addScopes(org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope value) {
        if (scopesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureScopesIsMutable();
          scopes_.add(value);
          onChanged();
        } else {
          scopesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .FamilyScope scopes = 6;</code>
       */
      public Builder addScopes(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope value) {
        if (scopesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureScopesIsMutable();
          scopes_.add(index, value);
          onChanged();
        } else {
          scopesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .FamilyScope scopes = 6;</code>
       */
      public Builder addScopes(
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder builderForValue) {
        if (scopesBuilder_ == null) {
          ensureScopesIsMutable();
          scopes_.add(builderForValue.build());
          onChanged();
        } else {
          scopesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .FamilyScope scopes = 6;</code>
       */
      public Builder addScopes(
          int index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder builderForValue) {
        if (scopesBuilder_ == null) {
          ensureScopesIsMutable();
          scopes_.add(index, builderForValue.build());
          onChanged();
        } else {
          scopesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .FamilyScope scopes = 6;</code>
       */
      public Builder addAllScopes(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope> values) {
        if (scopesBuilder_ == null) {
          ensureScopesIsMutable();
          super.addAll(values, scopes_);
          onChanged();
        } else {
          scopesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .FamilyScope scopes = 6;</code>
       */
      public Builder clearScopes() {
        if (scopesBuilder_ == null) {
          scopes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
          onChanged();
        } else {
          scopesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .FamilyScope scopes = 6;</code>
       */
      public Builder removeScopes(int index) {
        if (scopesBuilder_ == null) {
          ensureScopesIsMutable();
          scopes_.remove(index);
          onChanged();
        } else {
          scopesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .FamilyScope scopes = 6;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder getScopesBuilder(
          int index) {
        return getScopesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .FamilyScope scopes = 6;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder getScopesOrBuilder(
          int index) {
        if (scopesBuilder_ == null) {
          return scopes_.get(index);  } else {
          return scopesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .FamilyScope scopes = 6;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder> 
           getScopesOrBuilderList() {
        if (scopesBuilder_ != null) {
          return scopesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(scopes_);
        }
      }
      /**
       * <code>repeated .FamilyScope scopes = 6;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder addScopesBuilder() {
        return getScopesFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.getDefaultInstance());
      }
      /**
       * <code>repeated .FamilyScope scopes = 6;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder addScopesBuilder(
          int index) {
        return getScopesFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.getDefaultInstance());
      }
      /**
       * <code>repeated .FamilyScope scopes = 6;</code>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder> 
           getScopesBuilderList() {
        return getScopesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder> 
          getScopesFieldBuilder() {
        if (scopesBuilder_ == null) {
          scopesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder>(
                  scopes_,
                  ((bitField0_ & 0x00000020) == 0x00000020),
                  getParentForChildren(),
                  isClean());
          scopes_ = null;
        }
        return scopesBuilder_;
      }

      // optional uint32 following_kv_count = 7;
      private int followingKvCount_ ;
      /**
       * <code>optional uint32 following_kv_count = 7;</code>
       */
      public boolean hasFollowingKvCount() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional uint32 following_kv_count = 7;</code>
       */
      public int getFollowingKvCount() {
        return followingKvCount_;
      }
      /**
       * <code>optional uint32 following_kv_count = 7;</code>
       */
      public Builder setFollowingKvCount(int value) {
        bitField0_ |= 0x00000040;
        followingKvCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint32 following_kv_count = 7;</code>
       */
      public Builder clearFollowingKvCount() {
        bitField0_ = (bitField0_ & ~0x00000040);
        followingKvCount_ = 0;
        onChanged();
        return this;
      }

      // repeated .UUID cluster_ids = 8;
      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID> clusterIds_ =
        java.util.Collections.emptyList();
      private void ensureClusterIdsIsMutable() {
        if (!((bitField0_ & 0x00000080) == 0x00000080)) {
          clusterIds_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID>(clusterIds_);
          bitField0_ |= 0x00000080;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder> clusterIdsBuilder_;

      /**
       * <code>repeated .UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID> getClusterIdsList() {
        if (clusterIdsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(clusterIds_);
        } else {
          return clusterIdsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public int getClusterIdsCount() {
        if (clusterIdsBuilder_ == null) {
          return clusterIds_.size();
        } else {
          return clusterIdsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID getClusterIds(int index) {
        if (clusterIdsBuilder_ == null) {
          return clusterIds_.get(index);
        } else {
          return clusterIdsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public Builder setClusterIds(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID value) {
        if (clusterIdsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureClusterIdsIsMutable();
          clusterIds_.set(index, value);
          onChanged();
        } else {
          clusterIdsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public Builder setClusterIds(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder builderForValue) {
        if (clusterIdsBuilder_ == null) {
          ensureClusterIdsIsMutable();
          clusterIds_.set(index, builderForValue.build());
          onChanged();
        } else {
          clusterIdsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public Builder addClusterIds(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID value) {
        if (clusterIdsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureClusterIdsIsMutable();
          clusterIds_.add(value);
          onChanged();
        } else {
          clusterIdsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public Builder addClusterIds(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID value) {
        if (clusterIdsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureClusterIdsIsMutable();
          clusterIds_.add(index, value);
          onChanged();
        } else {
          clusterIdsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public Builder addClusterIds(
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder builderForValue) {
        if (clusterIdsBuilder_ == null) {
          ensureClusterIdsIsMutable();
          clusterIds_.add(builderForValue.build());
          onChanged();
        } else {
          clusterIdsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public Builder addClusterIds(
          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder builderForValue) {
        if (clusterIdsBuilder_ == null) {
          ensureClusterIdsIsMutable();
          clusterIds_.add(index, builderForValue.build());
          onChanged();
        } else {
          clusterIdsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public Builder addAllClusterIds(
          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID> values) {
        if (clusterIdsBuilder_ == null) {
          ensureClusterIdsIsMutable();
          super.addAll(values, clusterIds_);
          onChanged();
        } else {
          clusterIdsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public Builder clearClusterIds() {
        if (clusterIdsBuilder_ == null) {
          clusterIds_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000080);
          onChanged();
        } else {
          clusterIdsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public Builder removeClusterIds(int index) {
        if (clusterIdsBuilder_ == null) {
          ensureClusterIdsIsMutable();
          clusterIds_.remove(index);
          onChanged();
        } else {
          clusterIdsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder getClusterIdsBuilder(
          int index) {
        return getClusterIdsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder getClusterIdsOrBuilder(
          int index) {
        if (clusterIdsBuilder_ == null) {
          return clusterIds_.get(index);  } else {
          return clusterIdsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder> 
           getClusterIdsOrBuilderList() {
        if (clusterIdsBuilder_ != null) {
          return clusterIdsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(clusterIds_);
        }
      }
      /**
       * <code>repeated .UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder addClusterIdsBuilder() {
        return getClusterIdsFieldBuilder().addBuilder(
            org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance());
      }
      /**
       * <code>repeated .UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder addClusterIdsBuilder(
          int index) {
        return getClusterIdsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance());
      }
      /**
       * <code>repeated .UUID cluster_ids = 8;</code>
       *
       * <pre>
       *
       *This field contains the list of clusters that have
       *consumed the change
       * </pre>
       */
      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder> 
           getClusterIdsBuilderList() {
        return getClusterIdsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder> 
          getClusterIdsFieldBuilder() {
        if (clusterIdsBuilder_ == null) {
          clusterIdsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder>(
                  clusterIds_,
                  ((bitField0_ & 0x00000080) == 0x00000080),
                  getParentForChildren(),
                  isClean());
          clusterIds_ = null;
        }
        return clusterIdsBuilder_;
      }

      // optional uint64 nonceGroup = 9;
      private long nonceGroup_ ;
      /**
       * <code>optional uint64 nonceGroup = 9;</code>
       */
      public boolean hasNonceGroup() {
        return ((bitField0_ & 0x00000100) == 0x00000100);
      }
      /**
       * <code>optional uint64 nonceGroup = 9;</code>
       */
      public long getNonceGroup() {
        return nonceGroup_;
      }
      /**
       * <code>optional uint64 nonceGroup = 9;</code>
       */
      public Builder setNonceGroup(long value) {
        bitField0_ |= 0x00000100;
        nonceGroup_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 nonceGroup = 9;</code>
       */
      public Builder clearNonceGroup() {
        bitField0_ = (bitField0_ & ~0x00000100);
        nonceGroup_ = 0L;
        onChanged();
        return this;
      }

      // optional uint64 nonce = 10;
      private long nonce_ ;
      /**
       * <code>optional uint64 nonce = 10;</code>
       */
      public boolean hasNonce() {
        return ((bitField0_ & 0x00000200) == 0x00000200);
      }
      /**
       * <code>optional uint64 nonce = 10;</code>
       */
      public long getNonce() {
        return nonce_;
      }
      /**
       * <code>optional uint64 nonce = 10;</code>
       */
      public Builder setNonce(long value) {
        bitField0_ |= 0x00000200;
        nonce_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional uint64 nonce = 10;</code>
       */
      public Builder clearNonce() {
        bitField0_ = (bitField0_ & ~0x00000200);
        nonce_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:WALKey)
    }

    static {
      defaultInstance = new WALKey(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:WALKey)
  }

  public interface FamilyScopeOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required bytes family = 1;
    /**
     * <code>required bytes family = 1;</code>
     */
    boolean hasFamily();
    /**
     * <code>required bytes family = 1;</code>
     */
    com.google.protobuf.ByteString getFamily();

    // required .ScopeType scope_type = 2;
    /**
     * <code>required .ScopeType scope_type = 2;</code>
     */
    boolean hasScopeType();
    /**
     * <code>required .ScopeType scope_type = 2;</code>
     */
    org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType getScopeType();
  }
  /**
   * Protobuf type {@code FamilyScope}
   */
  public static final class FamilyScope extends
      com.google.protobuf.GeneratedMessage
      implements FamilyScopeOrBuilder {
    // Use FamilyScope.newBuilder() to construct.
    private FamilyScope(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private FamilyScope(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final FamilyScope defaultInstance;
    public static FamilyScope getDefaultInstance() {
      return defaultInstance;
    }

    public FamilyScope getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private FamilyScope(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              family_ = input.readBytes();
              break;
            }
            case 16: {
              int rawValue = input.readEnum();
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType value = org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(2, rawValue);
              } else {
                bitField0_ |= 0x00000002;
                scopeType_ = value;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_FamilyScope_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_FamilyScope_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder.class);
    }

    public static com.google.protobuf.Parser<FamilyScope> PARSER =
        new com.google.protobuf.AbstractParser<FamilyScope>() {
      public FamilyScope parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new FamilyScope(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<FamilyScope> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required bytes family = 1;
    public static final int FAMILY_FIELD_NUMBER = 1;
    private com.google.protobuf.ByteString family_;
    /**
     * <code>required bytes family = 1;</code>
     */
    public boolean hasFamily() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required bytes family = 1;</code>
     */
    public com.google.protobuf.ByteString getFamily() {
      return family_;
    }

    // required .ScopeType scope_type = 2;
    public static final int SCOPE_TYPE_FIELD_NUMBER = 2;
    private org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType scopeType_;
    /**
     * <code>required .ScopeType scope_type = 2;</code>
     */
    public boolean hasScopeType() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required .ScopeType scope_type = 2;</code>
     */
    public org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType getScopeType() {
      return scopeType_;
    }

    private void initFields() {
      family_ = com.google.protobuf.ByteString.EMPTY;
      scopeType_ = org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType.REPLICATION_SCOPE_LOCAL;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasFamily()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasScopeType()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, family_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeEnum(2, scopeType_.getNumber());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, family_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, scopeType_.getNumber());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope other = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope) obj;

      boolean result = true;
      result = result && (hasFamily() == other.hasFamily());
      if (hasFamily()) {
        result = result && getFamily()
            .equals(other.getFamily());
      }
      result = result && (hasScopeType() == other.hasScopeType());
      if (hasScopeType()) {
        result = result &&
            (getScopeType() == other.getScopeType());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasFamily()) {
        hash = (37 * hash) + FAMILY_FIELD_NUMBER;
        hash = (53 * hash) + getFamily().hashCode();
      }
      if (hasScopeType()) {
        hash = (37 * hash) + SCOPE_TYPE_FIELD_NUMBER;
        hash = (53 * hash) + hashEnum(getScopeType());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code FamilyScope}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScopeOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_FamilyScope_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_FamilyScope_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        family_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        scopeType_ = org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType.REPLICATION_SCOPE_LOCAL;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_FamilyScope_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope build() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope result = new org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.family_ = family_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.scopeType_ = scopeType_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope.getDefaultInstance()) return this;
        if (other.hasFamily()) {
          setFamily(other.getFamily());
        }
        if (other.hasScopeType()) {
          setScopeType(other.getScopeType());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasFamily()) {
          
          return false;
        }
        if (!hasScopeType()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.FamilyScope) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required bytes family = 1;
      private com.google.protobuf.ByteString family_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes family = 1;</code>
       */
      public boolean hasFamily() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required bytes family = 1;</code>
       */
      public com.google.protobuf.ByteString getFamily() {
        return family_;
      }
      /**
       * <code>required bytes family = 1;</code>
       */
      public Builder setFamily(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        family_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes family = 1;</code>
       */
      public Builder clearFamily() {
        bitField0_ = (bitField0_ & ~0x00000001);
        family_ = getDefaultInstance().getFamily();
        onChanged();
        return this;
      }

      // required .ScopeType scope_type = 2;
      private org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType scopeType_ = org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType.REPLICATION_SCOPE_LOCAL;
      /**
       * <code>required .ScopeType scope_type = 2;</code>
       */
      public boolean hasScopeType() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required .ScopeType scope_type = 2;</code>
       */
      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType getScopeType() {
        return scopeType_;
      }
      /**
       * <code>required .ScopeType scope_type = 2;</code>
       */
      public Builder setScopeType(org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000002;
        scopeType_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required .ScopeType scope_type = 2;</code>
       */
      public Builder clearScopeType() {
        bitField0_ = (bitField0_ & ~0x00000002);
        scopeType_ = org.apache.hadoop.hbase.protobuf.generated.WALProtos.ScopeType.REPLICATION_SCOPE_LOCAL;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:FamilyScope)
    }

    static {
      defaultInstance = new FamilyScope(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:FamilyScope)
  }

  public interface CompactionDescriptorOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required bytes table_name = 1;
    /**
     * <code>required bytes table_name = 1;</code>
     *
     * <pre>
     * TODO: WALKey already stores these, might remove
     * </pre>
     */
    boolean hasTableName();
    /**
     * <code>required bytes table_name = 1;</code>
     *
     * <pre>
     * TODO: WALKey already stores these, might remove
     * </pre>
     */
    com.google.protobuf.ByteString getTableName();

    // required bytes encoded_region_name = 2;
    /**
     * <code>required bytes encoded_region_name = 2;</code>
     */
    boolean hasEncodedRegionName();
    /**
     * <code>required bytes encoded_region_name = 2;</code>
     */
    com.google.protobuf.ByteString getEncodedRegionName();

    // required bytes family_name = 3;
    /**
     * <code>required bytes family_name = 3;</code>
     */
    boolean hasFamilyName();
    /**
     * <code>required bytes family_name = 3;</code>
     */
    com.google.protobuf.ByteString getFamilyName();

    // repeated string compaction_input = 4;
    /**
     * <code>repeated string compaction_input = 4;</code>
     */
    java.util.List<java.lang.String>
    getCompactionInputList();
    /**
     * <code>repeated string compaction_input = 4;</code>
     */
    int getCompactionInputCount();
    /**
     * <code>repeated string compaction_input = 4;</code>
     */
    java.lang.String getCompactionInput(int index);
    /**
     * <code>repeated string compaction_input = 4;</code>
     */
    com.google.protobuf.ByteString
        getCompactionInputBytes(int index);

    // repeated string compaction_output = 5;
    /**
     * <code>repeated string compaction_output = 5;</code>
     */
    java.util.List<java.lang.String>
    getCompactionOutputList();
    /**
     * <code>repeated string compaction_output = 5;</code>
     */
    int getCompactionOutputCount();
    /**
     * <code>repeated string compaction_output = 5;</code>
     */
    java.lang.String getCompactionOutput(int index);
    /**
     * <code>repeated string compaction_output = 5;</code>
     */
    com.google.protobuf.ByteString
        getCompactionOutputBytes(int index);

    // required string store_home_dir = 6;
    /**
     * <code>required string store_home_dir = 6;</code>
     */
    boolean hasStoreHomeDir();
    /**
     * <code>required string store_home_dir = 6;</code>
     */
    java.lang.String getStoreHomeDir();
    /**
     * <code>required string store_home_dir = 6;</code>
     */
    com.google.protobuf.ByteString
        getStoreHomeDirBytes();
  }
  /**
   * Protobuf type {@code CompactionDescriptor}
   *
   * <pre>
   **
   * Special WAL entry to hold all related to a compaction.
   * Written to WAL before completing compaction.  There is
   * sufficient info in the below message to complete later
   * the * compaction should we fail the WAL write.
   * </pre>
   */
  public static final class CompactionDescriptor extends
      com.google.protobuf.GeneratedMessage
      implements CompactionDescriptorOrBuilder {
    // Use CompactionDescriptor.newBuilder() to construct.
    private CompactionDescriptor(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private CompactionDescriptor(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final CompactionDescriptor defaultInstance;
    public static CompactionDescriptor getDefaultInstance() {
      return defaultInstance;
    }

    public CompactionDescriptor getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private CompactionDescriptor(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              tableName_ = input.readBytes();
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              encodedRegionName_ = input.readBytes();
              break;
            }
            case 26: {
              bitField0_ |= 0x00000004;
              familyName_ = input.readBytes();
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
                compactionInput_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000008;
              }
              compactionInput_.add(input.readBytes());
              break;
            }
            case 42: {
              if (!((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
                compactionOutput_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000010;
              }
              compactionOutput_.add(input.readBytes());
              break;
            }
            case 50: {
              bitField0_ |= 0x00000008;
              storeHomeDir_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
          compactionInput_ = new com.google.protobuf.UnmodifiableLazyStringList(compactionInput_);
        }
        if (((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
          compactionOutput_ = new com.google.protobuf.UnmodifiableLazyStringList(compactionOutput_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_CompactionDescriptor_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_CompactionDescriptor_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor.Builder.class);
    }

    public static com.google.protobuf.Parser<CompactionDescriptor> PARSER =
        new com.google.protobuf.AbstractParser<CompactionDescriptor>() {
      public CompactionDescriptor parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new CompactionDescriptor(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<CompactionDescriptor> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required bytes table_name = 1;
    public static final int TABLE_NAME_FIELD_NUMBER = 1;
    private com.google.protobuf.ByteString tableName_;
    /**
     * <code>required bytes table_name = 1;</code>
     *
     * <pre>
     * TODO: WALKey already stores these, might remove
     * </pre>
     */
    public boolean hasTableName() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required bytes table_name = 1;</code>
     *
     * <pre>
     * TODO: WALKey already stores these, might remove
     * </pre>
     */
    public com.google.protobuf.ByteString getTableName() {
      return tableName_;
    }

    // required bytes encoded_region_name = 2;
    public static final int ENCODED_REGION_NAME_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString encodedRegionName_;
    /**
     * <code>required bytes encoded_region_name = 2;</code>
     */
    public boolean hasEncodedRegionName() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required bytes encoded_region_name = 2;</code>
     */
    public com.google.protobuf.ByteString getEncodedRegionName() {
      return encodedRegionName_;
    }

    // required bytes family_name = 3;
    public static final int FAMILY_NAME_FIELD_NUMBER = 3;
    private com.google.protobuf.ByteString familyName_;
    /**
     * <code>required bytes family_name = 3;</code>
     */
    public boolean hasFamilyName() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>required bytes family_name = 3;</code>
     */
    public com.google.protobuf.ByteString getFamilyName() {
      return familyName_;
    }

    // repeated string compaction_input = 4;
    public static final int COMPACTION_INPUT_FIELD_NUMBER = 4;
    private com.google.protobuf.LazyStringList compactionInput_;
    /**
     * <code>repeated string compaction_input = 4;</code>
     */
    public java.util.List<java.lang.String>
        getCompactionInputList() {
      return compactionInput_;
    }
    /**
     * <code>repeated string compaction_input = 4;</code>
     */
    public int getCompactionInputCount() {
      return compactionInput_.size();
    }
    /**
     * <code>repeated string compaction_input = 4;</code>
     */
    public java.lang.String getCompactionInput(int index) {
      return compactionInput_.get(index);
    }
    /**
     * <code>repeated string compaction_input = 4;</code>
     */
    public com.google.protobuf.ByteString
        getCompactionInputBytes(int index) {
      return compactionInput_.getByteString(index);
    }

    // repeated string compaction_output = 5;
    public static final int COMPACTION_OUTPUT_FIELD_NUMBER = 5;
    private com.google.protobuf.LazyStringList compactionOutput_;
    /**
     * <code>repeated string compaction_output = 5;</code>
     */
    public java.util.List<java.lang.String>
        getCompactionOutputList() {
      return compactionOutput_;
    }
    /**
     * <code>repeated string compaction_output = 5;</code>
     */
    public int getCompactionOutputCount() {
      return compactionOutput_.size();
    }
    /**
     * <code>repeated string compaction_output = 5;</code>
     */
    public java.lang.String getCompactionOutput(int index) {
      return compactionOutput_.get(index);
    }
    /**
     * <code>repeated string compaction_output = 5;</code>
     */
    public com.google.protobuf.ByteString
        getCompactionOutputBytes(int index) {
      return compactionOutput_.getByteString(index);
    }

    // required string store_home_dir = 6;
    public static final int STORE_HOME_DIR_FIELD_NUMBER = 6;
    private java.lang.Object storeHomeDir_;
    /**
     * <code>required string store_home_dir = 6;</code>
     */
    public boolean hasStoreHomeDir() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>required string store_home_dir = 6;</code>
     */
    public java.lang.String getStoreHomeDir() {
      java.lang.Object ref = storeHomeDir_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          storeHomeDir_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string store_home_dir = 6;</code>
     */
    public com.google.protobuf.ByteString
        getStoreHomeDirBytes() {
      java.lang.Object ref = storeHomeDir_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        storeHomeDir_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      tableName_ = com.google.protobuf.ByteString.EMPTY;
      encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
      familyName_ = com.google.protobuf.ByteString.EMPTY;
      compactionInput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      compactionOutput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      storeHomeDir_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasTableName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasEncodedRegionName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasFamilyName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasStoreHomeDir()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, tableName_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, encodedRegionName_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBytes(3, familyName_);
      }
      for (int i = 0; i < compactionInput_.size(); i++) {
        output.writeBytes(4, compactionInput_.getByteString(i));
      }
      for (int i = 0; i < compactionOutput_.size(); i++) {
        output.writeBytes(5, compactionOutput_.getByteString(i));
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeBytes(6, getStoreHomeDirBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, tableName_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, encodedRegionName_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(3, familyName_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < compactionInput_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeBytesSizeNoTag(compactionInput_.getByteString(i));
        }
        size += dataSize;
        size += 1 * getCompactionInputList().size();
      }
      {
        int dataSize = 0;
        for (int i = 0; i < compactionOutput_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeBytesSizeNoTag(compactionOutput_.getByteString(i));
        }
        size += dataSize;
        size += 1 * getCompactionOutputList().size();
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(6, getStoreHomeDirBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor other = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor) obj;

      boolean result = true;
      result = result && (hasTableName() == other.hasTableName());
      if (hasTableName()) {
        result = result && getTableName()
            .equals(other.getTableName());
      }
      result = result && (hasEncodedRegionName() == other.hasEncodedRegionName());
      if (hasEncodedRegionName()) {
        result = result && getEncodedRegionName()
            .equals(other.getEncodedRegionName());
      }
      result = result && (hasFamilyName() == other.hasFamilyName());
      if (hasFamilyName()) {
        result = result && getFamilyName()
            .equals(other.getFamilyName());
      }
      result = result && getCompactionInputList()
          .equals(other.getCompactionInputList());
      result = result && getCompactionOutputList()
          .equals(other.getCompactionOutputList());
      result = result && (hasStoreHomeDir() == other.hasStoreHomeDir());
      if (hasStoreHomeDir()) {
        result = result && getStoreHomeDir()
            .equals(other.getStoreHomeDir());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasTableName()) {
        hash = (37 * hash) + TABLE_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getTableName().hashCode();
      }
      if (hasEncodedRegionName()) {
        hash = (37 * hash) + ENCODED_REGION_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getEncodedRegionName().hashCode();
      }
      if (hasFamilyName()) {
        hash = (37 * hash) + FAMILY_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getFamilyName().hashCode();
      }
      if (getCompactionInputCount() > 0) {
        hash = (37 * hash) + COMPACTION_INPUT_FIELD_NUMBER;
        hash = (53 * hash) + getCompactionInputList().hashCode();
      }
      if (getCompactionOutputCount() > 0) {
        hash = (37 * hash) + COMPACTION_OUTPUT_FIELD_NUMBER;
        hash = (53 * hash) + getCompactionOutputList().hashCode();
      }
      if (hasStoreHomeDir()) {
        hash = (37 * hash) + STORE_HOME_DIR_FIELD_NUMBER;
        hash = (53 * hash) + getStoreHomeDir().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code CompactionDescriptor}
     *
     * <pre>
     **
     * Special WAL entry to hold all related to a compaction.
     * Written to WAL before completing compaction.  There is
     * sufficient info in the below message to complete later
     * the * compaction should we fail the WAL write.
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptorOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_CompactionDescriptor_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_CompactionDescriptor_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        tableName_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        familyName_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000004);
        compactionInput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000008);
        compactionOutput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000010);
        storeHomeDir_ = "";
        bitField0_ = (bitField0_ & ~0x00000020);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_CompactionDescriptor_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor build() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor result = new org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.tableName_ = tableName_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.encodedRegionName_ = encodedRegionName_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.familyName_ = familyName_;
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          compactionInput_ = new com.google.protobuf.UnmodifiableLazyStringList(
              compactionInput_);
          bitField0_ = (bitField0_ & ~0x00000008);
        }
        result.compactionInput_ = compactionInput_;
        if (((bitField0_ & 0x00000010) == 0x00000010)) {
          compactionOutput_ = new com.google.protobuf.UnmodifiableLazyStringList(
              compactionOutput_);
          bitField0_ = (bitField0_ & ~0x00000010);
        }
        result.compactionOutput_ = compactionOutput_;
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000008;
        }
        result.storeHomeDir_ = storeHomeDir_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor.getDefaultInstance()) return this;
        if (other.hasTableName()) {
          setTableName(other.getTableName());
        }
        if (other.hasEncodedRegionName()) {
          setEncodedRegionName(other.getEncodedRegionName());
        }
        if (other.hasFamilyName()) {
          setFamilyName(other.getFamilyName());
        }
        if (!other.compactionInput_.isEmpty()) {
          if (compactionInput_.isEmpty()) {
            compactionInput_ = other.compactionInput_;
            bitField0_ = (bitField0_ & ~0x00000008);
          } else {
            ensureCompactionInputIsMutable();
            compactionInput_.addAll(other.compactionInput_);
          }
          onChanged();
        }
        if (!other.compactionOutput_.isEmpty()) {
          if (compactionOutput_.isEmpty()) {
            compactionOutput_ = other.compactionOutput_;
            bitField0_ = (bitField0_ & ~0x00000010);
          } else {
            ensureCompactionOutputIsMutable();
            compactionOutput_.addAll(other.compactionOutput_);
          }
          onChanged();
        }
        if (other.hasStoreHomeDir()) {
          bitField0_ |= 0x00000020;
          storeHomeDir_ = other.storeHomeDir_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasTableName()) {
          
          return false;
        }
        if (!hasEncodedRegionName()) {
          
          return false;
        }
        if (!hasFamilyName()) {
          
          return false;
        }
        if (!hasStoreHomeDir()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required bytes table_name = 1;
      private com.google.protobuf.ByteString tableName_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes table_name = 1;</code>
       *
       * <pre>
       * TODO: WALKey already stores these, might remove
       * </pre>
       */
      public boolean hasTableName() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required bytes table_name = 1;</code>
       *
       * <pre>
       * TODO: WALKey already stores these, might remove
       * </pre>
       */
      public com.google.protobuf.ByteString getTableName() {
        return tableName_;
      }
      /**
       * <code>required bytes table_name = 1;</code>
       *
       * <pre>
       * TODO: WALKey already stores these, might remove
       * </pre>
       */
      public Builder setTableName(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        tableName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes table_name = 1;</code>
       *
       * <pre>
       * TODO: WALKey already stores these, might remove
       * </pre>
       */
      public Builder clearTableName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        tableName_ = getDefaultInstance().getTableName();
        onChanged();
        return this;
      }

      // required bytes encoded_region_name = 2;
      private com.google.protobuf.ByteString encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes encoded_region_name = 2;</code>
       */
      public boolean hasEncodedRegionName() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required bytes encoded_region_name = 2;</code>
       */
      public com.google.protobuf.ByteString getEncodedRegionName() {
        return encodedRegionName_;
      }
      /**
       * <code>required bytes encoded_region_name = 2;</code>
       */
      public Builder setEncodedRegionName(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        encodedRegionName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes encoded_region_name = 2;</code>
       */
      public Builder clearEncodedRegionName() {
        bitField0_ = (bitField0_ & ~0x00000002);
        encodedRegionName_ = getDefaultInstance().getEncodedRegionName();
        onChanged();
        return this;
      }

      // required bytes family_name = 3;
      private com.google.protobuf.ByteString familyName_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes family_name = 3;</code>
       */
      public boolean hasFamilyName() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>required bytes family_name = 3;</code>
       */
      public com.google.protobuf.ByteString getFamilyName() {
        return familyName_;
      }
      /**
       * <code>required bytes family_name = 3;</code>
       */
      public Builder setFamilyName(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        familyName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes family_name = 3;</code>
       */
      public Builder clearFamilyName() {
        bitField0_ = (bitField0_ & ~0x00000004);
        familyName_ = getDefaultInstance().getFamilyName();
        onChanged();
        return this;
      }

      // repeated string compaction_input = 4;
      private com.google.protobuf.LazyStringList compactionInput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureCompactionInputIsMutable() {
        if (!((bitField0_ & 0x00000008) == 0x00000008)) {
          compactionInput_ = new com.google.protobuf.LazyStringArrayList(compactionInput_);
          bitField0_ |= 0x00000008;
         }
      }
      /**
       * <code>repeated string compaction_input = 4;</code>
       */
      public java.util.List<java.lang.String>
          getCompactionInputList() {
        return java.util.Collections.unmodifiableList(compactionInput_);
      }
      /**
       * <code>repeated string compaction_input = 4;</code>
       */
      public int getCompactionInputCount() {
        return compactionInput_.size();
      }
      /**
       * <code>repeated string compaction_input = 4;</code>
       */
      public java.lang.String getCompactionInput(int index) {
        return compactionInput_.get(index);
      }
      /**
       * <code>repeated string compaction_input = 4;</code>
       */
      public com.google.protobuf.ByteString
          getCompactionInputBytes(int index) {
        return compactionInput_.getByteString(index);
      }
      /**
       * <code>repeated string compaction_input = 4;</code>
       */
      public Builder setCompactionInput(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureCompactionInputIsMutable();
        compactionInput_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string compaction_input = 4;</code>
       */
      public Builder addCompactionInput(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureCompactionInputIsMutable();
        compactionInput_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string compaction_input = 4;</code>
       */
      public Builder addAllCompactionInput(
          java.lang.Iterable<java.lang.String> values) {
        ensureCompactionInputIsMutable();
        super.addAll(values, compactionInput_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string compaction_input = 4;</code>
       */
      public Builder clearCompactionInput() {
        compactionInput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000008);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string compaction_input = 4;</code>
       */
      public Builder addCompactionInputBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureCompactionInputIsMutable();
        compactionInput_.add(value);
        onChanged();
        return this;
      }

      // repeated string compaction_output = 5;
      private com.google.protobuf.LazyStringList compactionOutput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureCompactionOutputIsMutable() {
        if (!((bitField0_ & 0x00000010) == 0x00000010)) {
          compactionOutput_ = new com.google.protobuf.LazyStringArrayList(compactionOutput_);
          bitField0_ |= 0x00000010;
         }
      }
      /**
       * <code>repeated string compaction_output = 5;</code>
       */
      public java.util.List<java.lang.String>
          getCompactionOutputList() {
        return java.util.Collections.unmodifiableList(compactionOutput_);
      }
      /**
       * <code>repeated string compaction_output = 5;</code>
       */
      public int getCompactionOutputCount() {
        return compactionOutput_.size();
      }
      /**
       * <code>repeated string compaction_output = 5;</code>
       */
      public java.lang.String getCompactionOutput(int index) {
        return compactionOutput_.get(index);
      }
      /**
       * <code>repeated string compaction_output = 5;</code>
       */
      public com.google.protobuf.ByteString
          getCompactionOutputBytes(int index) {
        return compactionOutput_.getByteString(index);
      }
      /**
       * <code>repeated string compaction_output = 5;</code>
       */
      public Builder setCompactionOutput(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureCompactionOutputIsMutable();
        compactionOutput_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string compaction_output = 5;</code>
       */
      public Builder addCompactionOutput(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureCompactionOutputIsMutable();
        compactionOutput_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string compaction_output = 5;</code>
       */
      public Builder addAllCompactionOutput(
          java.lang.Iterable<java.lang.String> values) {
        ensureCompactionOutputIsMutable();
        super.addAll(values, compactionOutput_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string compaction_output = 5;</code>
       */
      public Builder clearCompactionOutput() {
        compactionOutput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000010);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string compaction_output = 5;</code>
       */
      public Builder addCompactionOutputBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureCompactionOutputIsMutable();
        compactionOutput_.add(value);
        onChanged();
        return this;
      }

      // required string store_home_dir = 6;
      private java.lang.Object storeHomeDir_ = "";
      /**
       * <code>required string store_home_dir = 6;</code>
       */
      public boolean hasStoreHomeDir() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>required string store_home_dir = 6;</code>
       */
      public java.lang.String getStoreHomeDir() {
        java.lang.Object ref = storeHomeDir_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          storeHomeDir_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string store_home_dir = 6;</code>
       */
      public com.google.protobuf.ByteString
          getStoreHomeDirBytes() {
        java.lang.Object ref = storeHomeDir_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          storeHomeDir_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string store_home_dir = 6;</code>
       */
      public Builder setStoreHomeDir(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        storeHomeDir_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string store_home_dir = 6;</code>
       */
      public Builder clearStoreHomeDir() {
        bitField0_ = (bitField0_ & ~0x00000020);
        storeHomeDir_ = getDefaultInstance().getStoreHomeDir();
        onChanged();
        return this;
      }
      /**
       * <code>required string store_home_dir = 6;</code>
       */
      public Builder setStoreHomeDirBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        storeHomeDir_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:CompactionDescriptor)
    }

    static {
      defaultInstance = new CompactionDescriptor(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:CompactionDescriptor)
  }

  public interface WALTrailerOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code WALTrailer}
   *
   * <pre>
   **
   * A trailer that is appended to the end of a properly closed HLog WAL file.
   * If missing, this is either a legacy or a corrupted WAL file.
   * </pre>
   */
  public static final class WALTrailer extends
      com.google.protobuf.GeneratedMessage
      implements WALTrailerOrBuilder {
    // Use WALTrailer.newBuilder() to construct.
    private WALTrailer(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private WALTrailer(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final WALTrailer defaultInstance;
    public static WALTrailer getDefaultInstance() {
      return defaultInstance;
    }

    public WALTrailer getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private WALTrailer(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_WALTrailer_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_WALTrailer_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer.Builder.class);
    }

    public static com.google.protobuf.Parser<WALTrailer> PARSER =
        new com.google.protobuf.AbstractParser<WALTrailer>() {
      public WALTrailer parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new WALTrailer(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<WALTrailer> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer other = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code WALTrailer}
     *
     * <pre>
     **
     * A trailer that is appended to the end of a properly closed HLog WAL file.
     * If missing, this is either a legacy or a corrupted WAL file.
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailerOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_WALTrailer_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_WALTrailer_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer.class, org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer.Builder.class);
      }

      // Construct using org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.internal_static_WALTrailer_descriptor;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer getDefaultInstanceForType() {
        return org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer.getDefaultInstance();
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer build() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer buildPartial() {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer result = new org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer) {
          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer other) {
        if (other == org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.WALProtos.WALTrailer) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:WALTrailer)
    }

    static {
      defaultInstance = new WALTrailer(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:WALTrailer)
  }

  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_WALHeader_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_WALHeader_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_WALKey_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_WALKey_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_FamilyScope_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_FamilyScope_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_CompactionDescriptor_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_CompactionDescriptor_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_WALTrailer_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_WALTrailer_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\tWAL.proto\032\013HBase.proto\"Y\n\tWALHeader\022\027\n" +
      "\017has_compression\030\001 \001(\010\022\026\n\016encryption_key" +
      "\030\002 \001(\014\022\033\n\023has_tag_compression\030\003 \001(\010\"\202\002\n\006" +
      "WALKey\022\033\n\023encoded_region_name\030\001 \002(\014\022\022\n\nt" +
      "able_name\030\002 \002(\014\022\033\n\023log_sequence_number\030\003" +
      " \002(\004\022\022\n\nwrite_time\030\004 \002(\004\022\035\n\ncluster_id\030\005" +
      " \001(\0132\005.UUIDB\002\030\001\022\034\n\006scopes\030\006 \003(\0132\014.Family" +
      "Scope\022\032\n\022following_kv_count\030\007 \001(\r\022\032\n\013clu" +
      "ster_ids\030\010 \003(\0132\005.UUID\022\022\n\nnonceGroup\030\t \001(" +
      "\004\022\r\n\005nonce\030\n \001(\004\"=\n\013FamilyScope\022\016\n\006famil",
      "y\030\001 \002(\014\022\036\n\nscope_type\030\002 \002(\0162\n.ScopeType\"" +
      "\251\001\n\024CompactionDescriptor\022\022\n\ntable_name\030\001" +
      " \002(\014\022\033\n\023encoded_region_name\030\002 \002(\014\022\023\n\013fam" +
      "ily_name\030\003 \002(\014\022\030\n\020compaction_input\030\004 \003(\t" +
      "\022\031\n\021compaction_output\030\005 \003(\t\022\026\n\016store_hom" +
      "e_dir\030\006 \002(\t\"\014\n\nWALTrailer*F\n\tScopeType\022\033" +
      "\n\027REPLICATION_SCOPE_LOCAL\020\000\022\034\n\030REPLICATI" +
      "ON_SCOPE_GLOBAL\020\001B?\n*org.apache.hadoop.h" +
      "base.protobuf.generatedB\tWALProtosH\001\210\001\000\240" +
      "\001\001"
    };
    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
      new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
        public com.google.protobuf.ExtensionRegistry assignDescriptors(
            com.google.protobuf.Descriptors.FileDescriptor root) {
          descriptor = root;
          internal_static_WALHeader_descriptor =
            getDescriptor().getMessageTypes().get(0);
          internal_static_WALHeader_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_WALHeader_descriptor,
              new java.lang.String[] { "HasCompression", "EncryptionKey", "HasTagCompression", });
          internal_static_WALKey_descriptor =
            getDescriptor().getMessageTypes().get(1);
          internal_static_WALKey_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_WALKey_descriptor,
              new java.lang.String[] { "EncodedRegionName", "TableName", "LogSequenceNumber", "WriteTime", "ClusterId", "Scopes", "FollowingKvCount", "ClusterIds", "NonceGroup", "Nonce", });
          internal_static_FamilyScope_descriptor =
            getDescriptor().getMessageTypes().get(2);
          internal_static_FamilyScope_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_FamilyScope_descriptor,
              new java.lang.String[] { "Family", "ScopeType", });
          internal_static_CompactionDescriptor_descriptor =
            getDescriptor().getMessageTypes().get(3);
          internal_static_CompactionDescriptor_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_CompactionDescriptor_descriptor,
              new java.lang.String[] { "TableName", "EncodedRegionName", "FamilyName", "CompactionInput", "CompactionOutput", "StoreHomeDir", });
          internal_static_WALTrailer_descriptor =
            getDescriptor().getMessageTypes().get(4);
          internal_static_WALTrailer_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_WALTrailer_descriptor,
              new java.lang.String[] { });
          return null;
        }
      };
    com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.getDescriptor(),
        }, assigner);
  }

  // @@protoc_insertion_point(outer_class_scope)
}
